# Lab 1: Building a Simple Chat Interface

In this lab, you will learn how to build chat interfaces that interact with Large Language Models using the OpenAI-compatible API. You'll work with local LLMs and understand the fundamentals of prompt engineering and conversation management.

## Learning Objectives

By the end of this lab, you will be able to:

- Set up a connection to a local or remote LLM using the OpenAI-compatible API
- Build a simple interactive chat application
- Implement asynchronous chat for better performance
- Use structured output for predictable responses

## Prerequisites

Make sure you have the required dependencies installed:

```bash
pip install openai
```

You'll also need a local LLM server running (e.g., Ollama) or access to an OpenAI-compatible API endpoint.

## Key Concepts

- **OpenAI-compatible API**: A standard API format for interacting with LLMs
- **System Prompt**: Instructions that define the AI assistant's behavior
- **User Message**: The input from the user in a conversation
- **Completion**: The response generated by the LLM
- **Temperature**: Controls randomness in responses (0 = deterministic, 1 = creative)

## Lab Exercises

### Exercise 1: Setting Up the Client

Navigate to the [examples/1-simple](../examples/1-simple/) directory.

1. Study [chat.py](../examples/1-simple/chat.py):

```python
import os
from openai import OpenAI

# Set up the client with environment variables
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE")
)
model_name = os.getenv("MODEL_NAME")
```

2. The client requires:
   - `OPENAI_API_KEY`: Authentication key (can be any value for local LLMs)
   - `OPENAI_API_BASE`: The API endpoint URL (e.g., `http://localhost:11434/v1`)
   - `MODEL_NAME`: The model to use (e.g., `qwen2.5-coder:7b-instruct`)

### Exercise 2: Building a Chat Function

Create a function to send messages and receive responses:

```python
def ai_chat(user_message):
    message_text = [
        {"role": "system", "content": "You are a friendly AI assistant that helps people find information and answer questions."},
        {"role": "user", "content": user_message}
    ]

    completion = client.chat.completions.create(
        model=model_name,
        messages=message_text,
        temperature=0.7,
        max_tokens=800,
        top_p=0.95,
    )
    return completion
```

### Exercise 3: Creating an Interactive Loop

Build an interactive chat loop:

```python
print("Welcome! How can I help you today?")

while True:
    user_message = input(">> ")
    completion = ai_chat(user_message)
    print(completion.choices[0].message.content)
```

Run the chat:

```bash
export MODEL_NAME="qwen2.5-coder:7b-instruct"
export OPENAI_API_BASE="http://localhost:11434/v1"
export OPENAI_API_KEY="ollama"
python chat.py
```

### Exercise 4: Async Chat for Better Performance

Study [async.py](../examples/1-simple/async.py) for asynchronous operations:

```python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE")
)

async def ai_chat(user_message):
    completion = await client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_message}
        ],
    )
    return completion.choices[0].message.content
```

Benefits of async:
- Non-blocking I/O while waiting for LLM responses
- Can handle multiple requests concurrently
- Better resource utilization

### Exercise 5: Structured Output

Study [structured.py](../examples/1-simple/structured.py) for predictable response formats:

```python
from pydantic import BaseModel

class MovieReview(BaseModel):
    title: str
    rating: int
    summary: str

completion = client.beta.chat.completions.parse(
    model=model_name,
    messages=[
        {"role": "system", "content": "Extract movie review information."},
        {"role": "user", "content": "I loved Inception! 5 stars. Mind-bending thriller."}
    ],
    response_format=MovieReview,
)

review = completion.choices[0].message.parsed
print(f"Title: {review.title}, Rating: {review.rating}")
```

### Exercise 6: Experimenting with Parameters

Experiment with different generation parameters:

```python
# More creative responses
completion = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.9,  # Higher = more random
    max_tokens=200,
)

# More focused responses
completion = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.2,  # Lower = more deterministic
    max_tokens=200,
)
```

## Challenge

1. Create a chat application with a custom system prompt for a specific use case (e.g., a coding tutor, recipe assistant, or travel planner)
2. Implement conversation history so the LLM remembers previous messages
3. Add error handling for when the LLM server is unavailable
4. Experiment with different temperature values and document the differences

## Summary

In this lab, you learned how to:
- Connect to LLMs using the OpenAI-compatible API
- Build interactive chat applications
- Use async operations for better performance
- Generate structured output from LLM responses
- Control generation parameters like temperature and max_tokens

## Next Steps

Continue to [Lab 2: Retrieval Augmented Generation (RAG)](./lab-2.md) to learn how to extend LLM capabilities with external data.
